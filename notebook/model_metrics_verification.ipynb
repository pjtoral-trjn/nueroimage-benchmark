{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Metrics Verification\n",
    "\n",
    "### Due to the private non-exportable nature of our dataset, this notebook is meant to verify the metrics shown in our report. Here you will see how each model is loaded from a saved state and evaluated on the test set. The importatnt verification cells appear halfway through the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec  8 13:50:11 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:04:00.0 Off |                  N/A |\n",
      "| 23%   36C    P0              60W / 250W |      0MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:06:00.0 Off |                  N/A |\n",
      "| 21%   36C    P0              59W / 250W |      0MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:07:00.0 Off |                  N/A |\n",
      "| 23%   40C    P0              59W / 250W |      0MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:08:00.0 Off |                  N/A |\n",
      "| 21%   38C    P0              59W / 250W |      0MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 21%   39C    P0              58W / 250W |      0MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 23%   42C    P0              60W / 250W |      0MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 18%   35C    P0              58W / 250W |      0MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:0F:00.0 Off |                  N/A |\n",
      "| 19%   39C    P0              58W / 250W |      0MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 13:50:16.851936: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.gridspec as gridspec\n",
    "import model.TCNN, model.VGG.tiny_vgg, model.Densenet.Densenet3D, model.ResNet.ResNet_Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1 , GPU ID:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 13:50:24.844198: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-12-08 13:50:24.846450: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-12-08 13:50:29.798838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:04:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-12-08 13:50:29.798900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-12-08 13:50:29.803626: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-12-08 13:50:29.803680: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-12-08 13:50:29.807261: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-12-08 13:50:29.808885: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-12-08 13:50:29.812835: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-12-08 13:50:29.815406: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-12-08 13:50:29.822521: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-12-08 13:50:29.823067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices), \", GPU ID: \", \"0\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset From Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.train_pathway = \"\"\n",
    "        self.test_pathway = \"\"\n",
    "        self.pathway = \"/lfs1/pjtoral/cognitive-decline/scripts/data/revised/standardized/mci_included\"\n",
    "        self.dof = \"9DOF\"\n",
    "        self.target_column = \"MMSE\"\n",
    "        self.batch_size = 1\n",
    "        # self.transformation = self.args.transformation\n",
    "        # self.sample_weight = self.args.sample_weight\n",
    "        # if self.sample_weight == \"dense_weight\":\n",
    "        #     self.dense_weight_alpha = experiment_config[\"alpha\"]\n",
    "\n",
    "        self.train_df = pd.DataFrame()\n",
    "        self.validation_df = pd.DataFrame()\n",
    "        self.test_df = pd.DataFrame()\n",
    "        self.train_batch = pd.DataFrame()\n",
    "        self.validation_batch = pd.DataFrame()\n",
    "        self.test_batch = pd.DataFrame()\n",
    "\n",
    "        self.set_dataframes()\n",
    "        self.set_data_generators()\n",
    "\n",
    "    def set_dataframes(self):\n",
    "        df_train_ADNI1 = pd.read_csv(self.pathway + \"/train_ADNI1_\" + self.dof + \".csv\")\n",
    "        df_train_ADNI2 = pd.read_csv(self.pathway + \"/train_ADNI2_\" + self.dof + \".csv\")\n",
    "        df_train_ADNI3 = pd.read_csv(self.pathway + \"/train_ADNI3_\" + self.dof + \".csv\")\n",
    "        df_test_ADNI1 = pd.read_csv(self.pathway + \"/test_ADNI1_\" + self.dof + \".csv\")\n",
    "        df_test_ADNI2 = pd.read_csv(self.pathway + \"/test_ADNI2_\" + self.dof + \".csv\")\n",
    "        df_test_ADNI3 = pd.read_csv(self.pathway + \"/test_ADNI3_\" + self.dof + \".csv\")\n",
    "        df_train = pd.concat([df_train_ADNI1, df_train_ADNI2, df_train_ADNI3], ignore_index=True).reset_index(drop=True)\n",
    "        df_test = pd.concat([df_test_ADNI1, df_test_ADNI2, df_test_ADNI3], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "        # df_train = pd.read_csv(self.train_pathway)\n",
    "        # df_test = pd.read_csv(self.test_pathway)\n",
    "\n",
    "        df_train.dropna(subset=[self.target_column], inplace=True)\n",
    "        df_test.dropna(subset=[self.target_column], inplace=True)\n",
    "        self.test_df = df_test\n",
    "        self.split_dataframes(df_train)\n",
    "\n",
    "    def split_dataframes(self, df_train):\n",
    "        df_train.reset_index(inplace=True)\n",
    "        df_validation = pd.DataFrame()\n",
    "        # ADNI\n",
    "        df_train[\"subj_id\"] = [\"_\".join(x.split(\"/\")[-1].split(\"_\")[:3]) for x in df_train['volume']]\n",
    "        sgkf = StratifiedGroupKFold(n_splits=2, shuffle=True, random_state=7)\n",
    "        (train_idxs, validation_idxs) = next(\n",
    "            sgkf.split(df_train.drop(columns=[\"label\"]), df_train[\"label\"], groups=df_train[\"subj_id\"]))\n",
    "        df_train_ = df_train.iloc[train_idxs]\n",
    "        df_validation = df_train.iloc[validation_idxs]\n",
    "\n",
    "        sgkf.split(df_validation.drop(columns=[\"label\"]), df_validation[\"label\"], groups=df_validation[\"subj_id\"])\n",
    "        (train_addition_idx, true_validation_idxs) = next(\n",
    "            sgkf.split(df_validation.drop(columns=[\"label\"]), df_validation[\"label\"], groups=df_validation[\"subj_id\"]))\n",
    "        df_train_ = pd.concat([df_train_, df_validation.iloc[train_addition_idx]], ignore_index=True)\n",
    "        df_validation = df_validation.iloc[true_validation_idxs]\n",
    "\n",
    "        df_train = df_train_.copy()\n",
    "        self.train_df = df_train\n",
    "        self.validation_df = df_validation\n",
    "\n",
    "    def set_data_generators(self):\n",
    "        train_x = self.train_df[\"volume\"].to_numpy()\n",
    "        train_y = self.train_df[self.target_column].to_numpy().astype(np.float32)\n",
    "        validate_x = self.validation_df[\"volume\"].to_numpy()\n",
    "        validate_y = self.validation_df[self.target_column].to_numpy().astype(np.float32)\n",
    "        test_x = self.test_df[\"volume\"].to_numpy()\n",
    "        test_y = self.test_df[self.target_column].to_numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "        # if self.sample_weight == \"dense_weight\":\n",
    "        #     dw = DenseWeight(alpha=self.dense_weight_alpha)\n",
    "        #     sample_weights = dw.fit(train_y)\n",
    "        #     self.train_batch = self.DataGenerator(train_x, train_y, self.batch_size, sample_weights)\n",
    "\n",
    "        self.train_batch = self.DataGenerator(train_x, train_y, self.batch_size)\n",
    "        self.validation_batch = self.DataGenerator(validate_x, validate_y, self.batch_size)\n",
    "        self.test_batch = self.DataGenerator(test_x, test_y, self.batch_size)\n",
    "\n",
    "    class DataGenerator(tf.keras.utils.Sequence):\n",
    "        def read_scan(self, path):\n",
    "            scan = nib.load(path)\n",
    "            original_volume = scan.get_fdata()\n",
    "            original_volume_normalized = self.normalize(original_volume)\n",
    "            resized_volume = self.resize(original_volume_normalized)\n",
    "            return tf.expand_dims(resized_volume, axis=3)\n",
    "\n",
    "        def normalize(self, volume):\n",
    "            min = np.amax(volume)\n",
    "            max = np.amin(volume)\n",
    "            volume = (volume - min) / (max - min)\n",
    "            volume = volume.astype(\"float32\")\n",
    "            return volume\n",
    "\n",
    "        def resize(self, original_volume, w=96, h=96, d=96):\n",
    "            zoom_factors = (w / original_volume.shape[0], h / original_volume.shape[1], d / original_volume.shape[2])\n",
    "            resized_volume = zoom(original_volume, zoom_factors)\n",
    "            resized_volume_normalized = self.normalize(resized_volume)\n",
    "            return resized_volume_normalized\n",
    "\n",
    "        def display(self):\n",
    "            path = self.image_filenames[0]\n",
    "            scan = nib.load(path)\n",
    "            print(path)\n",
    "            original_volume = scan.get_fdata()\n",
    "            original_volume_normalized = self.normalize(original_volume)\n",
    "            resized_volume = self.resize(original_volume_normalized)\n",
    "        \n",
    "            # Get the middle slice of the original image\n",
    "            original_slice = original_volume[:, :, original_volume.shape[2] // 2]\n",
    "            # Get the middle slice of the resized image\n",
    "            resized_slice = resized_volume[:, :, resized_volume.shape[2] // 2]\n",
    "            #\n",
    "            # Plot the slices\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.imshow(original_slice, cmap='gray')\n",
    "            plt.title('MMSE Score: '+str(self.labels[0]))\n",
    "            plt.savefig('mri.png')\n",
    "#             plt.show()\n",
    "            plt.imshow(resized_slice, cmap='gray')\n",
    "            plt.title('Resized Image ' + str(resized_volume.shape),)\n",
    "#             plt.show()\n",
    "\n",
    "        def __init__(self, image_filenames, labels, batch_size, sample_weights=None):\n",
    "            self.image_filenames = image_filenames\n",
    "            self.labels = labels\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        def __len__(self):\n",
    "            return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            batch_x = self.image_filenames[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "            batch_y = self.labels[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "            return (np.asarray([self.read_scan(path) for path in batch_x]), np.array(batch_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Test Set from the Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"drop_out\":0.0}\n",
    "data = Data(args)\n",
    "test_batch = data.test_batch\n",
    "\n",
    "train_mean = np.mean(data.train_df[\"MMSE\"])\n",
    "train_std = np.std(data.train_df[\"MMSE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load All Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'drop_out': 0.0}\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GroupNormalization.call of <tensorflow_addons.layers.normalizations.InstanceNormalization object at 0x7f13d4403950>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method GroupNormalization.call of <tensorflow_addons.layers.normalizations.InstanceNormalization object at 0x7f13d4403950>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 13:50:30.433535: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-08 13:50:30.439683: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-12-08 13:50:30.440199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:04:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-12-08 13:50:30.440261: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-12-08 13:50:30.440293: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-12-08 13:50:30.440327: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-12-08 13:50:30.440347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-12-08 13:50:30.440367: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-12-08 13:50:30.440388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-12-08 13:50:30.440409: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-12-08 13:50:30.440429: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-12-08 13:50:30.440795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-12-08 13:50:30.440839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-12-08 13:50:30.952356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-12-08 13:50:30.952406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-12-08 13:50:30.952419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-12-08 13:50:30.953226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10270 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "print(args)\n",
    "resnet_baseline = model.ResNet.ResNet_Conv.RESNET3D(args, train_mean, train_std).get_model(False)\n",
    "resnet_improvement = model.ResNet.ResNet_Conv.RESNET3D(args, train_mean, train_std).get_model(False)\n",
    "vgg_baseline = model.VGG.tiny_vgg.TinyVGG(args, train_mean, train_std).get_model(False)\n",
    "vgg_improvement = model.VGG.tiny_vgg.TinyVGG(args, train_mean, train_std).get_model(False)\n",
    "tcnn_baseline = model.TCNN.TCNN(args, train_mean, train_std).get_model(False)\n",
    "tcnn_improvement = model.TCNN.TCNN(args, train_mean, train_std).get_model(False)\n",
    "densenet_baseline = model.Densenet.Densenet3D.DenseNet3D(args, train_mean, train_std, depth=121, nb_dense_block=4,\n",
    "                                        growth_rate=32,\n",
    "                                        nb_filter=64, nb_layers_per_block=[6, 12, 24, 16],\n",
    "                                        bottleneck=False, reduction=0.0,\n",
    "                                        dropout_rate=0.0, weight_decay=1e-4,\n",
    "                                        subsample_initial_block=True, include_top=False,\n",
    "                                        input_shape=(96, 96, 96, 1), pooling=\"max\")\n",
    "densenet_improvement = model.Densenet.Densenet3D.DenseNet3D(args, train_mean, train_std, depth=121, nb_dense_block=4,\n",
    "                                        growth_rate=32,\n",
    "                                        nb_filter=64, nb_layers_per_block=[6, 12, 24, 16],\n",
    "                                        bottleneck=False, reduction=0.0,\n",
    "                                        dropout_rate=0.0, weight_decay=1e-4,\n",
    "                                        subsample_initial_block=True, include_top=False,\n",
    "                                        input_shape=(96, 96, 96, 1), pooling=\"max\")\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=0.005,\n",
    "                                                  weight_decay=0.0001) \n",
    "loss_fn = tf.keras.losses.MeanSquaredError(\n",
    "                reduction=tf.keras.losses.Reduction.AUTO,\n",
    "                name='mean_squared_error'\n",
    "            )\n",
    "metrics = [\"mse\", \"mae\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Predictions on Test Set from Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "/nas/home/pjtoral/.conda/envs/pjt/lib/python3.7/site-packages/ipykernel_launcher.py:132: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "2023-12-08 13:50:37.713695: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-12-08 13:50:37.714429: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199895000 Hz\n",
      "2023-12-08 13:50:38.170539: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-12-08 13:50:38.662776: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-12-08 13:50:41.825988: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2023-12-08 13:50:41.897435: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet Cold-Start Inference Time: 1041.3665878772736 seconds\n",
      "Resnet Fine-Tuned Inference Time: 1026.49955868721 seconds\n"
     ]
    }
   ],
   "source": [
    "resnet_baseline.set_weights(tf.keras.models.load_model(\"../output/resnet_baseline__2023-11-27_12:37:39/save\", compile=False).get_weights())\n",
    "resnet_improvement.set_weights(tf.keras.models.load_model(\"../output/resnet_tl_improvement__2023-12-03_13:05:18/save\", compile=False).get_weights())\n",
    "resnet_baseline.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=metrics\n",
    "        )\n",
    "resnet_improvement.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "start = time.time()\n",
    "resnet_baseline_predictions = [p[0] for p in resnet_baseline.predict(test_batch)]\n",
    "end = time.time()\n",
    "print(\"Resnet Cold-Start Inference Time: \"+ str(end-start) + \" seconds\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "resnet_improvement_predictions = [p[0] for p in resnet_improvement.predict(test_batch)]\n",
    "end = time.time()\n",
    "print(\"Resnet Fine-Tuned Inference Time: \"+ str(end-start) + \" seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "/nas/home/pjtoral/.conda/envs/pjt/lib/python3.7/site-packages/ipykernel_launcher.py:132: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny VGG Cold-Start Inference Time: 1003.48930311203 seconds\n",
      "Tiny VGG Fine-Tuned Inference Time: 1005.3480904102325 seconds\n"
     ]
    }
   ],
   "source": [
    "vgg_baseline.set_weights(tf.keras.models.load_model(\"../output/vgg_baseline_no_wd__2023-11-24_11:26:27/save\", compile=False).get_weights())\n",
    "vgg_improvement.set_weights(tf.keras.models.load_model(\"../output/tiny_vgg_tl_improvement__2023-12-06_13:05:25/save\", compile=False).get_weights())\n",
    "vgg_baseline.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=metrics\n",
    "        )\n",
    "vgg_improvement.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "start = time.time()\n",
    "vgg_baseline_predictions = [p[0] for p in vgg_baseline.predict(test_batch)]\n",
    "end = time.time()\n",
    "print(\"Tiny VGG Cold-Start Inference Time: \"+ str(end-start) + \" seconds\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "vgg_improvement_predictions = [p[0] for p in vgg_improvement.predict(test_batch)]\n",
    "end = time.time()\n",
    "print(\"Tiny VGG Fine-Tuned Inference Time: \"+ str(end-start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "/nas/home/pjtoral/.conda/envs/pjt/lib/python3.7/site-packages/ipykernel_launcher.py:132: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCNN Cold-Start Inference Time: 1013.9330520629883 seconds\n",
      "TCNN Fine-Tuned Inference Time: 1024.0299062728882 seconds\n"
     ]
    }
   ],
   "source": [
    "tcnn_baseline.set_weights(tf.keras.models.load_model(\"../output/tcnn_baseline_no_wd__2023-11-24_11:27:16/save\", compile=False).get_weights())\n",
    "tcnn_improvement.set_weights(tf.keras.models.load_model(\"../output/tcnn_tl_improvement__2023-11-30_13:40:13/save\", compile=False).get_weights())\n",
    "tcnn_baseline.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=metrics\n",
    "        )\n",
    "tcnn_improvement.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "start = time.time()\n",
    "tcnn_baseline_predictions = [p[0] for p in tcnn_baseline.predict(test_batch)]\n",
    "end = time.time()\n",
    "print(\"TCNN Cold-Start Inference Time: \"+ str(end-start) + \" seconds\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "tcnn_improvement_predictions = [p[0] for p in tcnn_improvement.predict(test_batch)]\n",
    "end = time.time()\n",
    "print(\"TCNN Fine-Tuned Inference Time: \"+ str(end-start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/home/pjtoral/.conda/envs/pjt/lib/python3.7/site-packages/ipykernel_launcher.py:132: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Densenet Cold-Start Inference Time: 1050.1901426315308 seconds\n",
      "Densenet Fine-Tuned Inference Time: 1002.3589644432068 seconds\n"
     ]
    }
   ],
   "source": [
    "densenet_baseline.set_weights(tf.keras.models.load_model(\"../output/densenet_baseline_no_wd__2023-11-24_11:26:17/save\", compile=False).get_weights())\n",
    "densenet_improvement.set_weights(tf.keras.models.load_model(\"../output/densenet_tl_improvement__2023-11-30_13:53:48/save\", compile=False).get_weights())\n",
    "densenet_baseline.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=metrics\n",
    "        )\n",
    "densenet_improvement.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "start = time.time()\n",
    "densenet_baseline_predictions = [p[0] for p in densenet_baseline.predict(test_batch)]\n",
    "end = time.time()\n",
    "print(\"Densenet Cold-Start Inference Time: \"+ str(end-start) + \" seconds\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "densenet_improvement_predictions = [p[0] for p in densenet_improvement.predict(test_batch)]\n",
    "end = time.time()\n",
    "print(\"Densenet Fine-Tuned Inference Time: \"+ str(end-start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet Cold-Start\n",
      "MSE: 8.633\n",
      "MAE:  2.106\n",
      "R2:  0.353\n",
      "\n",
      "Resnet Fine-Tuned\n",
      "MSE: 8.341\n",
      "MAE:  2.1\n",
      "R2:  0.375\n"
     ]
    }
   ],
   "source": [
    "test_set = data.test_df[\"MMSE\"].values\n",
    "print(\"Resnet Cold-Start\")\n",
    "print(\"MSE:\",round(mean_squared_error(test_set,resnet_baseline_predictions),3))\n",
    "print(\"MAE: \",round(mean_absolute_error(test_set,resnet_baseline_predictions),3))\n",
    "print(\"R2: \",round(r2_score(test_set,resnet_baseline_predictions),3))\n",
    "\n",
    "print(\"\\nResnet Fine-Tuned\")\n",
    "print(\"MSE:\",round(mean_squared_error(test_set,resnet_improvement_predictions),3))\n",
    "print(\"MAE: \",round(mean_absolute_error(test_set,resnet_improvement_predictions),3))\n",
    "print(\"R2: \",round(r2_score(test_set,resnet_improvement_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCNN Cold-Start\n",
      "MSE: 9.333\n",
      "MAE:  2.219\n",
      "R2:  0.301\n",
      "\n",
      "TCNN Fine-Tuned\n",
      "MSE: 8.871\n",
      "MAE:  2.138\n",
      "R2:  0.335\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"TCNN Cold-Start\")\n",
    "print(\"MSE:\",round(mean_squared_error(test_set,tcnn_baseline_predictions),3))\n",
    "print(\"MAE: \",round(mean_absolute_error(test_set,tcnn_baseline_predictions),3))\n",
    "print(\"R2: \",round(r2_score(test_set,tcnn_baseline_predictions),3))\n",
    "\n",
    "print(\"\\nTCNN Fine-Tuned\")\n",
    "print(\"MSE:\",round(mean_squared_error(test_set,tcnn_improvement_predictions),3))\n",
    "print(\"MAE: \",round(mean_absolute_error(test_set,tcnn_improvement_predictions),3))\n",
    "print(\"R2: \",round(r2_score(test_set,tcnn_improvement_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny VGG Cold-Start\n",
      "MSE: 8.929\n",
      "MAE:  2.118\n",
      "R2:  0.331\n",
      "\n",
      "Tiny VGG Fine-Tuned\n",
      "MSE: 8.565\n",
      "MAE:  2.09\n",
      "R2:  0.358\n"
     ]
    }
   ],
   "source": [
    "print(\"Tiny VGG Cold-Start\")\n",
    "print(\"MSE:\",round(mean_squared_error(test_set,vgg_baseline_predictions),3))\n",
    "print(\"MAE: \",round(mean_absolute_error(test_set,vgg_baseline_predictions),3))\n",
    "print(\"R2: \",round(r2_score(test_set,vgg_baseline_predictions),3))\n",
    "\n",
    "print(\"\\nTiny VGG Fine-Tuned\")\n",
    "print(\"MSE:\",round(mean_squared_error(test_set,vgg_improvement_predictions),3))\n",
    "print(\"MAE: \",round(mean_absolute_error(test_set,vgg_improvement_predictions),3))\n",
    "print(\"R2: \",round(r2_score(test_set,vgg_improvement_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Densenet Cold-Start\n",
      "MSE: 10.652\n",
      "MAE:  2.244\n",
      "R2:  0.202\n",
      "\n",
      "DensenetFine-Tuned\n",
      "MSE: 9.394\n",
      "MAE:  2.091\n",
      "R2:  0.296\n"
     ]
    }
   ],
   "source": [
    "print(\"Densenet Cold-Start\")\n",
    "print(\"MSE:\",round(mean_squared_error(test_set,densenet_baseline_predictions),3))\n",
    "print(\"MAE: \",round(mean_absolute_error(test_set,densenet_baseline_predictions),3))\n",
    "print(\"R2: \",round(r2_score(test_set,densenet_baseline_predictions),3))\n",
    "\n",
    "print(\"\\nDensenetFine-Tuned\")\n",
    "print(\"MSE:\",round(mean_squared_error(test_set,densenet_improvement_predictions),3))\n",
    "print(\"MAE: \",round(mean_absolute_error(test_set,densenet_improvement_predictions),3))\n",
    "print(\"R2: \",round(r2_score(test_set,densenet_improvement_predictions),3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pjt",
   "language": "python",
   "name": "pjt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
